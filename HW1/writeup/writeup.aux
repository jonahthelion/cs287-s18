\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Description}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model and Algorithms}{1}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Affect of global smoothing parameter $\alpha $ on validation accuracy. Validation accuracy is calculated using sklearn's \texttt  {accuracy\_score}. The performance is not sensitive to $\alpha $.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clusters}{{1}{2}{Affect of global smoothing parameter $\alpha $ on validation accuracy. Validation accuracy is calculated using sklearn's \texttt {accuracy\_score}. The performance is not sensitive to $\alpha $.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Naive Bayes}{2}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }}{2}{table.caption.2}}
\newlabel{tab:nb}{{1}{2}{Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Words with the highest and lowest weights in the naive bayes weight vector agree with intuition for being negative and positive words respectively.\relax }}{2}{table.caption.3}}
\newlabel{tab:nbwords}{{2}{2}{Words with the highest and lowest weights in the naive bayes weight vector agree with intuition for being negative and positive words respectively.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Logistic Regression}{2}{subsection.4.2}}
\bibstyle{apalike}
\bibdata{writeup}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The left is the binary cross entropy recorded during training of binarized logistic regression and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is no better than naive bayes.\relax }}{3}{figure.caption.4}}
\newlabel{fig:logtrain}{{2}{3}{The left is the binary cross entropy recorded during training of binarized logistic regression and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is no better than naive bayes.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }}{3}{table.caption.5}}
\newlabel{tab:logreg}{{3}{3}{Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Words with the highest and lowest weights in the logistic regression weight vector agree with intuition for being negative and positive words respectively. There is some overlap with naive bayes.\relax }}{3}{table.caption.6}}
\newlabel{tab:logwords}{{4}{3}{Words with the highest and lowest weights in the logistic regression weight vector agree with intuition for being negative and positive words respectively. There is some overlap with naive bayes.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Continuous bag of Words}{3}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  The word vectors improve the accuracy of the model.\relax }}{4}{table.caption.7}}
\newlabel{tab:cbowtab}{{5}{4}{The word vectors improve the accuracy of the model.\relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The left is the binary cross entropy recorded during training of CBOW and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is significantly better than naive bayes.\relax }}{4}{figure.caption.8}}
\newlabel{fig:logtrain}{{3}{4}{The left is the binary cross entropy recorded during training of CBOW and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is significantly better than naive bayes.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}CNN}{4}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}ResNet}{4}{subsection.4.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}{section.5}}
