\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Description}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model and Algorithms}{1}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The left is the binary cross entropy recorded during training of binarized logistic regression and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is no better than naive bayes.\relax }}{2}{figure.caption.3}}
\newlabel{fig:logtrain}{{1}{2}{The left is the binary cross entropy recorded during training of binarized logistic regression and the right is the classification accuracy tracked during training. The objective function is tightly correlated with classification accuracy. The model is no better than naive bayes.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Naive Bayes}{2}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }}{2}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:nb}{{1}{2}{Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }{table.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Words with the highest and lowest weights in the naive bayes weight vector agree with intuition for being negative and positive words respectively.\relax }}{2}{table.caption.2}}
\newlabel{tab:nbwords}{{2}{2}{Words with the highest and lowest weights in the naive bayes weight vector agree with intuition for being negative and positive words respectively.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Logistic Regression}{2}{subsection.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }}{3}{table.caption.4}}
\newlabel{tab:logreg}{{3}{3}{Binarizing or counting words does not significantly affect the performance of the model on the test set.\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Words with the highest and lowest weights in the logistic regression weight vector agree with intuition for being negative and positive words respectively. There is some overlap with naive bayes.\relax }}{3}{table.caption.5}}
\newlabel{tab:logwords}{{4}{3}{Words with the highest and lowest weights in the logistic regression weight vector agree with intuition for being negative and positive words respectively. There is some overlap with naive bayes.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Continuous bag of Words}{3}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  The word vectors improve the accuracy of the model.\relax }}{3}{table.caption.6}}
\newlabel{tab:cbowtab}{{5}{3}{The word vectors improve the accuracy of the model.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}CNN}{3}{subsection.4.4}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Higher quality word vectors improve the model.\relax }}{4}{table.caption.7}}
\newlabel{tab:convtab}{{6}{4}{Higher quality word vectors improve the model.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}ResNet}{4}{subsection.4.5}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  ResNet learns features straight from the image of the text which generalize to near-naive bayes accuracy on the test set. \relax }}{4}{table.caption.8}}
\newlabel{tab:resres}{{7}{4}{ResNet learns features straight from the image of the text which generalize to near-naive bayes accuracy on the test set. \relax }{table.caption.8}{}}
\bibstyle{apalike}
\bibdata{writeup}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{section.5}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces  \relax }}{5}{table.caption.9}}
\newlabel{tab:conc}{{8}{5}{\relax }{table.caption.9}{}}
